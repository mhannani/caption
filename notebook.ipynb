{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f254454-d76e-45d1-b00d-d9b99e000965",
   "metadata": {},
   "source": [
    "# Image Captioning using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306c170-eae9-4a7e-9ee5-ab9b5f4f1b8e",
   "metadata": {},
   "source": [
    "### 1. Auto-reload by default for modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d084cc4-62a9-40fd-904f-47211c09ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f47370-21f2-49e9-96a7-701abe9d0fb9",
   "metadata": {},
   "source": [
    "### 2. Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cbc9d8aa-be68-4ac3-89af-2c193603b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from utils.data_loader import data_loader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.models import Captioner\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ead40-83d4-47e3-b790-b5d461acc8d8",
   "metadata": {},
   "source": [
    "### 3. Exlpore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bbc6045-1152-4d78-ad54-9db6f32580c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = pd.read_csv(\"Data/caption_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36264bac-cda7-4437-a6bf-2a4ea33213dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b545093-814c-4c67-b2d8-be445926dec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1107471216_4336c9b328.jpg</td>\n",
       "      <td>A little girl is holding a cine camera in fron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1107471216_4336c9b328.jpg</td>\n",
       "      <td>A young girl is looking through an old fashion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1107471216_4336c9b328.jpg</td>\n",
       "      <td>A young girl steadies her aim with a camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1107471216_4336c9b328.jpg</td>\n",
       "      <td>Girl with rosy cheeks and lips holding black t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1107471216_4336c9b328.jpg</td>\n",
       "      <td>There is a kid with a gun .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         image  \\\n",
       "0    1000268201_693b08cb0e.jpg   \n",
       "1    1000268201_693b08cb0e.jpg   \n",
       "2    1000268201_693b08cb0e.jpg   \n",
       "3    1000268201_693b08cb0e.jpg   \n",
       "4    1000268201_693b08cb0e.jpg   \n",
       "..                         ...   \n",
       "495  1107471216_4336c9b328.jpg   \n",
       "496  1107471216_4336c9b328.jpg   \n",
       "497  1107471216_4336c9b328.jpg   \n",
       "498  1107471216_4336c9b328.jpg   \n",
       "499  1107471216_4336c9b328.jpg   \n",
       "\n",
       "                                               caption  \n",
       "0    A child in a pink dress is climbing up a set o...  \n",
       "1                A girl going into a wooden building .  \n",
       "2     A little girl climbing into a wooden playhouse .  \n",
       "3    A little girl climbing the stairs to her playh...  \n",
       "4    A little girl in a pink dress going into a woo...  \n",
       "..                                                 ...  \n",
       "495  A little girl is holding a cine camera in fron...  \n",
       "496  A young girl is looking through an old fashion...  \n",
       "497        A young girl steadies her aim with a camera  \n",
       "498  Girl with rosy cheeks and lips holding black t...  \n",
       "499                        There is a kid with a gun .  \n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e085098-2ff6-4e2c-8647-bf479cdd8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "little_girl = Image.open(\"Data/Images/train/1000268201_693b08cb0e.jpg\")\n",
    "little_girl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bf7e2-005c-4a17-90d0-2b0e63a4060f",
   "metadata": {},
   "source": [
    "### 4. Make transformation for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b07c347c-24dc-40e6-8ebb-3bc7eee081ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((356, 356)),\n",
    "        transforms.RandomCrop((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7b5e1-4bc5-4b43-ae9a-fe60c660db91",
   "metadata": {},
   "source": [
    "### 5. Getting the training data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a942a84b-f708-46e2-b378-6e7eb0034303",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader__training, train_dataset = data_loader(root_dir=\"./Data/Images/train\",\n",
    "                                caption_file=\"./Data/caption_train.csv\",\n",
    "                                transform=img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28d6262f-1489-4833-ae92-f46b247051be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 3, 299, 299])\n",
      "torch.Size([23, 32])\n"
     ]
    }
   ],
   "source": [
    "for i, (image, caption) in enumerate(data_loader__training):\n",
    "    print(i)\n",
    "    print(image.shape)\n",
    "    print(caption.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bca76cb8-216d-4753-a06d-1abf48bcf950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Train the captioner\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Training\")\n",
    "    # Apply some transformation to our data\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # get the data\n",
    "    training_data, train_dataset = data_loader(root_dir=\"./Data/Images/train\",\n",
    "                                               caption_file=\"./Data/caption_train.csv\",\n",
    "                                               transform=transform, num_workers=6)\n",
    "\n",
    "    # get the test data\n",
    "    test_data, test_dataset = data_loader(root_dir=\"./Data/Images/test\",\n",
    "                                          caption_file=\"./Data/caption_test.csv\",\n",
    "                                          transform=transform, num_workers=6)\n",
    "\n",
    "    # get validation data\n",
    "    valid_data, valid_dataset = data_loader(root_dir=\"./Data/Images/valid\",\n",
    "                                            caption_file=\"./Data/caption_valid.csv\",\n",
    "                                            transform=transform, num_workers=6)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    load_model = False\n",
    "    save_model = True\n",
    "\n",
    "    # hyperparameters\n",
    "    embed_size = 256\n",
    "    hidden_size = 256\n",
    "    vocabulary_size = len(train_dataset.vocabulary)\n",
    "    num_layer = 1\n",
    "    lr = 3e-4\n",
    "    num_epochs = 101\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(\"runs/flickr\")\n",
    "    step = 0\n",
    "\n",
    "    # Initialize model\n",
    "    model = Captioner(embed_size, hidden_size, vocabulary_size, num_layer).to(device)\n",
    "\n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocabulary.stoi[\"<PAD>\"])\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # load checkpoint if already saved\n",
    "    if load_model:\n",
    "        step = load_checkpoint(torch.load(\"checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # training process\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for index, (images, captions) in enumerate(training_data):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            output = model(images, captions[:-1])\n",
    "\n",
    "            loss = criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "\n",
    "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            # accumulate the training loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            print(f'going through batches the current epoch {epoch}/{num_epochs}')\n",
    "            print(f\"current batch {index} / {len(training_data)}\")\n",
    "\n",
    "        print(f'Epoch: {epoch + 1}/{num_epochs} ... Training loss: {running_loss / len(training_data)}')\n",
    "\n",
    "        # save the model at this stage\n",
    "        if save_model:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step\n",
    "            }\n",
    "        # save model each 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                save_checkpoint(checkpoint, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a71f5104-fe74-4060-8389-6bd819a59987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "going through batches the current epoch 0/101\n",
      "current batch 0 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 1 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 2 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 3 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 4 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 5 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 6 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 7 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 8 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 9 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 10 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 11 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 12 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 13 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 14 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 15 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 16 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 17 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 18 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 19 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 20 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 21 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 22 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 23 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 24 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 25 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 26 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 27 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 28 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 29 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 30 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 31 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 32 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 33 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 34 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 35 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 36 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 37 / 796\n",
      "going through batches the current epoch 0/101\n",
      "current batch 38 / 796\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-529afb180e5f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/caption/utils/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/caption/utils/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# extract the features from the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# make the last linear layer trainable by enabling gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInceptionOutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_7b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# N x 2048 x 8 x 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_7c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;31m# N x 2048 x 8 x 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# Adaptive average pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mbranch3x3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3x3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         branch3x3 = [\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3x3_2a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch3x3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3x3_2b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch3x3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47f85e-b702-4553-8d6d-44a49cde101f",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ec5f3a1-b897-4ed0-bb27-179cef8ba658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f25319-93b0-442c-a989-7416be86adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/captions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9145be48-871d-458d-a3a7-b6979f1410af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b74044-b514-4560-8048-cf3ee4c44fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df[35455:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aadf4073-7c42-4bda-b727-0b5a41b759e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35455</th>\n",
       "      <td>378453580_21d688748e.jpg</td>\n",
       "      <td>A dog is jumping over a log in a wooded area w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35456</th>\n",
       "      <td>378453580_21d688748e.jpg</td>\n",
       "      <td>A dog with a stick in his mouth jumps over a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35457</th>\n",
       "      <td>378453580_21d688748e.jpg</td>\n",
       "      <td>Dog carries stick and jumps over a log .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35458</th>\n",
       "      <td>378453580_21d688748e.jpg</td>\n",
       "      <td>The dog carries a stick and jumps over a log i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35459</th>\n",
       "      <td>378453580_21d688748e.jpg</td>\n",
       "      <td>The dog jumps over the log with a stick in its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image  \\\n",
       "35455  378453580_21d688748e.jpg   \n",
       "35456  378453580_21d688748e.jpg   \n",
       "35457  378453580_21d688748e.jpg   \n",
       "35458  378453580_21d688748e.jpg   \n",
       "35459  378453580_21d688748e.jpg   \n",
       "...                         ...   \n",
       "40450  997722733_0cb5439472.jpg   \n",
       "40451  997722733_0cb5439472.jpg   \n",
       "40452  997722733_0cb5439472.jpg   \n",
       "40453  997722733_0cb5439472.jpg   \n",
       "40454  997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "35455  A dog is jumping over a log in a wooded area w...  \n",
       "35456  A dog with a stick in his mouth jumps over a f...  \n",
       "35457           Dog carries stick and jumps over a log .  \n",
       "35458  The dog carries a stick and jumps over a log i...  \n",
       "35459  The dog jumps over the log with a stick in its...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8977fafa-8074-4bf2-812a-ecc3e78853e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images_array = list(set(df_valid[\"image\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "724cef9b-7f14-4b0b-af51-12747db8a31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_valid.to_csv(\"Data/caption_valid.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6698785d-a0c9-4ace-bebf-33b3ec9f983b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "630dd747-9326-4b4f-bed6-585da8b8dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in valid_images_array:\n",
    "    os.rename(f\"Data/Images/{image}\", f\"Data/Images/valid/{image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82aefc49-cfbf-4cd3-a85e-ccdd6ee6039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[25455:35455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44b3b294-a70e-4190-a50c-ce92c73c4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"Data/caption_test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90fb74c3-f516-4790-861c-149287520e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "072855d4-da1c-43bb-a485-c33ceddf16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_array = list(set(df_test[\"image\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "673c6233-b1a1-4837-b590-5035b8820b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0516c448-95f4-4f31-904f-2cdad08b6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in test_images_array:\n",
    "    os.rename(f\"Data/Images/{image}\", f\"Data/Images/test/{image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d5762b5-4342-4fd8-a55e-b711cfb38aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:25455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af5bdfa7-ab13-4849-be0a-7adb694442f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25455"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0db3b7fa-bd9d-4e5b-9eff-992d9da2beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"Data/caption_train.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05580f55-f847-4f7b-a31c-4bfbc52e1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_array = list(set(df_train[\"image\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c01a392f-aaa9-4320-a789-3314347d4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in train_images_array:\n",
    "    os.rename(f\"Data/Images/{image}\", f\"Data/Images/train/{image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b16e32-3cf3-4abc-ad62-1fe2d1d14dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
